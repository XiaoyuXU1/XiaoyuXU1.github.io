---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span id='index'></span>

# About Me  

I am a Ph.D. candidate at **The Hong Kong Polytechnic University**, co-supervised by [**Prof. Haibo Hu**](https://haibohu.org/) and [**Dr. Minxin Du**](https://duminxin.github.io/). My research focuses on the **security, privacy, and reliability of large language models (LLMs)**. Before starting my Ph.D., I received my **B.Eng. in Electronics and Communication Engineering** from **Sun Yat-sen University (SYSU)** in 2024.  

---

## Research Interests  

- **Machine Unlearning for Large Language Models (LLMs):**  
  Designing efficient and robust algorithms that enable LLMs to selectively *forget* specific data while preserving their overall capability.  My research explores two key scenarios: (i) **Single Unlearning:** Removing a specific set of data or knowledge from a pre-trained model in a one-time unlearning request. (ii) **Continual Unlearning:** Extending unlearning to long sequences of deletion requests, addressing catastrophic forgetting.  

- **LLM Security and Trustworthiness:**  
  Investigating the robustness of LLMs against unlearning attacks like relearning and quantization perturbations to ensure safety and integrity in real-world applications.  

---

  
<!--<span class="anchor" id="news"></span>-->
# News

- **August 2025** — Our paper is accepted to EMNLP 2025 (Main Conference)!
- **June 2025** — Invited to review for [ACM Transactions on the Web (TWEB)](https://dl.acm.org/journal/tweb).
- **June 2025** — Invited by [EMNLP](https://2025.emnlp.org/) to serve as a secondary (external) reviewer.
- **May 2025** — My personal homepage officially launched!

<!--<span class="anchor" id="publications"></span>-->
# Publications 
- **Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning**  
  arXiv preprint: [2507.00432](https://arxiv.org/abs/2507.00432) • [PDF](https://arxiv.org/pdf/2507.00432)  
 Maggie Huan, Yuetai Li, Tuney Zheng, **Xiaoyu Xu**, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, Xiang Yue

- **Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs**  
  arXiv preprint: [2505.16831](https://arxiv.org/abs/2505.16831) • [PDF](https://arxiv.org/pdf/2505.16831.pdf)  
  **Xiaoyu Xu**, Xiang Yue, Yang Liu, Qingqing Ye, Haibo Hu, Minxin Du
  
- **OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models**  
  EMNLP 2025 (Main Conference): [2505.04416](https://arxiv.org/abs/2505.04416) • [PDF](https://arxiv.org/pdf/2505.04416.pdf)  
  **Xiaoyu Xu**, Minxin Du, Qingqing Ye, Haibo Hu
  
# Educations
- *2020.09 – 2024.06*, Bachelor of Engineering in Electronics and Communication Engineering, Sun Yat-sen University (SYSU)  
- *2024.09 – present*, Ph.D. Candidate, The Hong Kong Polytechnic University 


# Interests
- Fitness enthusiast: Passionate about strength training, cardio, and maintaining a healthy lifestyle.  
- Music lover: Enjoy listening to Mandopop artists such as Jay Chou, G.E.M., JJ Lin, and David Tao.  

 <a href="https://clustrmaps.com/site/1c73j"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=EsVhO2oJdnAEt2aXO6s9mWYkyN16gPxWAWyR7ALlNyc&cl=ffffff" /></a>
