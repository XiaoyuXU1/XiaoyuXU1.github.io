---
permalink: /home
title: "Home"
excerpt: ""

---
<span id='home'></span>

# About Me  

I am a Ph.D. candidate at **The Hong Kong Polytechnic University**, co-supervised by [**Prof. Haibo Hu**](https://haibohu.org/) and [**Dr. Minxin Du**](https://duminxin.github.io/).  
My research focuses on the **security, privacy, and reliability of large language models (LLMs)**.  
Before starting my Ph.D., I received my **B.Eng. in Electronics and Communication Engineering** from **Sun Yat-sen University (SYSU)** in 2024.  

---

## Research Interests  

- **Machine Unlearning for Large Language Models (LLMs):**  
  Designing efficient and robust algorithms that enable LLMs to selectively *forget* specific data or behaviors while preserving their overall capability.  
  My research explores two key scenarios:  
  - **Single Unlearning:** Removing a specific set of data or knowledge from a pre-trained model in a one-time unlearning request, ensuring minimal performance degradation.  
  - **Continual Unlearning:** Extending unlearning to long sequences of deletion requests, addressing cumulative forgetting, stability, and reversibility issues over time.  

- **LLM Security and Trustworthiness:**  
  Investigating the robustness of LLMs against unlearning attacks like relearning and quantization perturbations to ensure safety and integrity in real-world applications.  

---

  
<!--<span class="anchor" id="news"></span>-->
# News

- **August 2025** ‚Äî Our paper is accepted to EMNLP 2025 (Main Conference)!
- **June 2025** ‚Äî Invited to review for [ACM Transactions on the Web (TWEB)](https://dl.acm.org/journal/tweb).
- **June 2025** ‚Äî Invited by [EMNLP](https://2025.emnlp.org/) to serve as a secondary (external) reviewer.
- **May 2025** ‚Äî My personal homepage officially launched!

<!--<span class="anchor" id="publications"></span>-->
# üìù Publications 
- **Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning**  
  arXiv preprint: [2507.00432](https://arxiv.org/abs/2507.00432) ‚Ä¢ [PDF](https://arxiv.org/pdf/2507.00432)  
 Maggie Huan, Yuetai Li, Tuney Zheng, **Xiaoyu Xu**, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, Xiang Yue

- **Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs**  
  arXiv preprint: [2505.16831](https://arxiv.org/abs/2505.16831) ‚Ä¢ [PDF](https://arxiv.org/pdf/2505.16831.pdf)  
  **Xiaoyu Xu**, Xiang Yue, Yang Liu, Qingqing Ye, Haibo Hu, Minxin Du
  
- **OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models**  
  EMNLP 2025 (Main Conference): [2505.04416](https://arxiv.org/abs/2505.04416) ‚Ä¢ [PDF](https://arxiv.org/pdf/2505.04416.pdf)  
  **Xiaoyu Xu**, Minxin Du, Qingqing Ye, Haibo Hu
  
# Educations
- *2020.09 ‚Äì 2024.06*, Bachelor of Engineering in Electronics and Communication Engineering, Sun Yat-sen University (SYSU)  
- *2024.09 ‚Äì present*, Ph.D. Candidate, The Hong Kong Polytechnic University 


# Interests
- Fitness enthusiast: Passionate about strength training, cardio, and maintaining a healthy lifestyle.  
- Music lover: Enjoy listening to Mandopop artists such as Jay Chou, G.E.M., JJ Lin, and David Tao.  

 <a href="https://clustrmaps.com/site/1c73j"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=EsVhO2oJdnAEt2aXO6s9mWYkyN16gPxWAWyR7ALlNyc&cl=ffffff" /></a>
